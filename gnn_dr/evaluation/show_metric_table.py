#!/usr/bin/env python
"""
Display a metric table from baseline evaluation results.

Reads a CSV file generated by run_full_evaluation.py and displays a pivot table
with datasets as rows and methods as columns for a specific metric.

Usage:
    python -m gnn_dr.evaluation.show_metric_table results/baselines.csv Trustworthiness
    python -m gnn_dr.evaluation.show_metric_table results/baselines.csv Continuity --format latex
    python -m gnn_dr.evaluation.show_metric_table results/baselines.csv --list-metrics
"""

import argparse
import sys
from pathlib import Path

import pandas as pd


def list_available_metrics(df: pd.DataFrame) -> list:
    """List all available metrics in the DataFrame."""
    # Exclude standard columns
    exclude_cols = {'dataset', 'method', 'n_samples', 'n_classes', 'time_seconds', 
                    'fit_time', 'inference_time', 'error'}
    metrics = [col for col in df.columns if col not in exclude_cols]
    return sorted(metrics)


def format_value(val, precision: int = 4) -> str:
    """Format a numeric value for display."""
    if pd.isna(val):
        return '-'
    if isinstance(val, float):
        return f'{val:.{precision}f}'
    return str(val)


def print_metric_table(
    csv_path: str,
    metric: str,
    output_format: str = 'plain',
    precision: int = 4,
) -> None:
    """
    Print a pivot table for a specific metric.
    
    Args:
        csv_path: Path to the CSV file
        metric: Name of the metric column to display
        output_format: 'plain', 'markdown', 'latex', or 'csv'
        precision: Number of decimal places
    """
    # Load data
    df = pd.read_csv(csv_path)
    
    # Check if metric exists
    available_metrics = list_available_metrics(df)
    if metric not in df.columns:
        print(f"Error: Metric '{metric}' not found in CSV.")
        print(f"\nAvailable metrics ({len(available_metrics)}):")
        for m in available_metrics:
            print(f"  - {m}")
        sys.exit(1)
    
    # Create pivot table: datasets as rows, methods as columns
    pivot = df.pivot_table(
        values=metric,
        index='dataset',
        columns='method',
        aggfunc='first'  # Should be one value per dataset/method
    )
    
    # Sort columns in a sensible order
    method_order = ['PCA', 't-SNE', 'UMAP', 'Parametric_UMAP']
    existing_methods = [m for m in method_order if m in pivot.columns]
    other_methods = [m for m in pivot.columns if m not in method_order]
    pivot = pivot[existing_methods + other_methods]
    
    # Print header
    print(f"\n{'='*60}")
    print(f"Metric: {metric}")
    print(f"{'='*60}\n")
    
    if output_format == 'latex':
        # LaTeX table format
        latex_str = pivot.to_latex(
            float_format=f'%.{precision}f',
            na_rep='-',
            escape=True
        )
        print(latex_str)
    
    elif output_format == 'markdown':
        # Markdown table format
        md_str = pivot.to_markdown(floatfmt=f'.{precision}f')
        print(md_str)
    
    elif output_format == 'csv':
        # CSV format (useful for further processing)
        pivot.to_csv(sys.stdout)
    
    else:
        # Plain text format with nice alignment
        # Format all values
        formatted = pivot.map(lambda x: format_value(x, precision))
        
        # Print with nice formatting
        print(formatted.to_string())
    
    # Print summary statistics
    print(f"\n{'='*60}")
    print("Summary Statistics:")
    print(f"{'='*60}")
    
    for method in pivot.columns:
        mean_val = pivot[method].mean()
        std_val = pivot[method].std()
        print(f"  {method:20s}: mean={format_value(mean_val, precision)}, std={format_value(std_val, precision)}")
    
    print()


def main():
    parser = argparse.ArgumentParser(
        description='Display a metric table from baseline evaluation results',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python -m gnn_dr.evaluation.show_metric_table results/baselines.csv Trustworthiness
  python -m gnn_dr.evaluation.show_metric_table results/baselines.csv Continuity --format latex
  python -m gnn_dr.evaluation.show_metric_table results/baselines.csv --list-metrics
        """
    )
    parser.add_argument(
        'csv_path',
        type=str,
        help='Path to the CSV file from run_full_evaluation.py'
    )
    parser.add_argument(
        'metric',
        type=str,
        nargs='?',
        default=None,
        help='Name of the metric to display (e.g., Trustworthiness, Continuity)'
    )
    parser.add_argument(
        '--format', '-f',
        type=str,
        choices=['plain', 'markdown', 'latex', 'csv'],
        default='plain',
        help='Output format (default: plain)'
    )
    parser.add_argument(
        '--precision', '-p',
        type=int,
        default=4,
        help='Number of decimal places (default: 4)'
    )
    parser.add_argument(
        '--list-metrics', '-l',
        action='store_true',
        help='List all available metrics in the CSV'
    )
    
    args = parser.parse_args()
    
    # Check file exists
    if not Path(args.csv_path).exists():
        print(f"Error: File not found: {args.csv_path}")
        sys.exit(1)
    
    # Load data
    df = pd.read_csv(args.csv_path)
    
    # List metrics if requested
    if args.list_metrics or args.metric is None:
        metrics = list_available_metrics(df)
        print(f"\nAvailable metrics ({len(metrics)}):")
        print("="*40)
        for m in metrics:
            print(f"  {m}")
        print()
        
        if args.metric is None:
            print("Usage: python -m gnn_dr.evaluation.show_metric_table CSV_PATH METRIC")
        return
    
    # Print the table
    print_metric_table(
        args.csv_path,
        args.metric,
        output_format=args.format,
        precision=args.precision,
    )


if __name__ == '__main__':
    main()
